{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Oyn2Hp-XhDxIZqoXLYhLRtlvvYzT1Wm9",
      "authorship_tag": "ABX9TyPa0nt49hpJl1HeI78ThGai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsmackie/TalkingToWALS/blob/mainline/HidingBehindWALS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VpMvHAYRQf9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7718c9-d1ba-43e0-e97c-997fd6451db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m983.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    cohere \\\n",
        "    langchain \\\n",
        "    tiktoken \\\n",
        "    pinecone-client \\\n",
        "    langchain-openai \\\n",
        "    langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from uuid import uuid4\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "from pinecone import Pinecone, PodSpec, PineconeApiException\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
        "from langchain_openai import ChatOpenAI as LangChainChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings as LangChainOpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore as LangChainPinecone\n",
        "\n"
      ],
      "metadata": {
        "id": "QCvpIbFUu5NN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(text, disallowed_special=())\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "id": "gdRNyCSLDI1A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_chapter_metadata(path):\n",
        "  with open(os.path.join(path)) as f:\n",
        "    headers = f.readline().strip().split(',')\n",
        "    lines = [line.strip().split(',') for line in f]\n",
        "  chapter_metadata = dict()\n",
        "  for i,line in enumerate(lines):\n",
        "    chapter_metadata[i] = {'chapter number': i+1,\n",
        "                          'title': line[headers.index('Title')],\n",
        "                          'authors': line[headers.index('Contributors')],\n",
        "                          'linguistic subfield': line[headers.index('Area')]}\n",
        "  return chapter_metadata"
      ],
      "metadata": {
        "id": "B6ojEvRgJ7OC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_splitter_create_and_upsert_vectors(index,\n",
        "                                            path,\n",
        "                                            chapter_metadata,\n",
        "                                            text_splitter,\n",
        "                                            embeddings,\n",
        "                                            batch_limit=100,\n",
        "                                            subdirectory='chapters'):\n",
        "  batch_limit = batch_limit\n",
        "  texts = []\n",
        "  metadatas = []\n",
        "  for i,file in enumerate(os.listdir(os.path.join(path, subdirectory))):\n",
        "    metadata = chapter_metadata[i]\n",
        "    with open(os.path.join(path, subdirectory, file), mode='r') as f:\n",
        "      text = ''.join([line for line in f])\n",
        "    record_texts = text_splitter.split_text(text)\n",
        "    record_metadatas = [{\"chunk\": j, \"text\": text, **metadata}\n",
        "                          for j, text in enumerate(record_texts)]\n",
        "    texts.extend(record_texts)\n",
        "    metadatas.extend(record_metadatas)\n",
        "    if len(texts) >= batch_limit:\n",
        "      ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "      embeds = embeddings.embed_documents(texts)\n",
        "      index.upsert(vectors=zip(ids, embeds, metadatas))\n",
        "      texts = []\n",
        "      metadatas = []\n"
      ],
      "metadata": {
        "id": "qOyU15H3tov7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def html_splitter_create_and_upsert_vectors(index,\n",
        "                                            path,\n",
        "                                            chapter_metadata,\n",
        "                                            html_splitter,\n",
        "                                            embeddings):\n",
        "  batch_limit=100\n",
        "  texts = list()\n",
        "  metadatas = list()\n",
        "  for i,file in enumerate(os.listdir(path)):\n",
        "    #print(file)\n",
        "    if not file.endswith('html'):\n",
        "      continue\n",
        "    html_splits = html_splitter.split_text_from_file(os.path.join(path, file))\n",
        "    #Note: this might be too large, and maybe should be split again with RecursiveTextSplitter?\n",
        "    ids = [str(uuid4()) for _ in range(len(html_splits))]\n",
        "    embeds = embeddings.embed_documents([split.page_content for split in html_splits])\n",
        "    #metadatas = [split.metadata for split in html_splits]\n",
        "    metadatas = {key:value for key,value in chapter_metadata[i].items()}\n",
        "    for split in html_splits:\n",
        "      metadatas.update(split.metadata)\n",
        "    record_metadatas = [{\"chunk\": j, \"text\": split.page_content, **metadatas}\n",
        "                          for j, split in enumerate(html_splits)]\n",
        "    index.upsert(vectors=zip(ids, embeds, record_metadatas))"
      ],
      "metadata": {
        "id": "acB4OJyb7Uiw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def json_splitter_create_and_upsert_vectors(index, path, chapter_metadata, embeddings):\n",
        "  texts = list()\n",
        "  for i,file in enumerate(sorted(os.listdir(path))):\n",
        "    if not file.endswith('.json'):\n",
        "      continue\n",
        "    print(i, file)\n",
        "    metadatas = {key:value for key,value in chapter_metadata[i].items()}\n",
        "    text_preamble = ''.join(['Hints: ',\n",
        "                            ','.join([f'{k}={v}' for (k,v) in metadatas.items()]),\n",
        "                            '\\n'])\n",
        "    with open(os.path.join(path, file), encoding='utf-8', mode='r') as f:\n",
        "      json_file = json.load(f)\n",
        "    for blob in json_file:\n",
        "      chapter_metadata[i]['section'] = list()\n",
        "      try:\n",
        "        chapter_metadata[i]['section'].append(blob['section'].strip())\n",
        "      except KeyError:\n",
        "        chapter_metadata[i]['section'].append('1. Introduction')\n",
        "      for p in blob['paragraphs']:\n",
        "        texts.append('\\n'.join([text_preamble, p['text']]))\n",
        "    embeds = embeddings.embed_documents(texts)\n",
        "\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    records = [{\"chunk\": j, \"text\": text, **metadatas}\n",
        "                          for j, text in enumerate(texts)]\n",
        "    try:\n",
        "      index.upsert(vectors=zip(ids, embeds, records))\n",
        "    except PineconeApiException:\n",
        "      print('Pinecone exception occured. File is bigger than 2MB! Ignoring.')"
      ],
      "metadata": {
        "id": "0eGMOtWJZvyo"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsert_wals_data(index, embeddings):\n",
        "  #Get table of contents and other general metadata about WALS\n",
        "  with open('/content/drive/MyDrive/WALS/chapter_metadata.csv', encoding='utf-8') as file:\n",
        "    file_content = file.read()\n",
        "  embeds = embeddings.embed_documents([file_content])\n",
        "  vector = [{\n",
        "      \"id\":str(uuid4()),\n",
        "      \"values\": embeds[0],\n",
        "      \"metadata\": {\"topic\": \"table of contents\"}\n",
        "    }]\n",
        "  index.upsert(vector)"
      ],
      "metadata": {
        "id": "bfLN1SomhnZ5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsert_language_data(index, embeddings):\n",
        "  with open('/content/drive/MyDrive/WALS/languages.csv') as f:\n",
        "    headers = f.readline().strip().split(',')\n",
        "    rows = [row.strip().split(',') for row in f]\n",
        "  blobs = list()\n",
        "  for row in rows:\n",
        "    blobs.append({headers[j]:value for (j,value) in enumerate(row)})\n",
        "  #There are ~3000 languages, which makes too many individual records\n",
        "  #try grouping them by family\n",
        "\n",
        "  families = list(set([language['Family'] for language in blobs]))\n",
        "  for family in families:\n",
        "    full_text = list()\n",
        "\n",
        "    family_matches = [language for language in blobs if language['Family'] == family]\n",
        "    genera = list(set([language['Genus'] for language in family_matches]))\n",
        "    full_text.append(f'Language Family: {family} has {len(genera)} subdivisions/groups/genera/etc.\\n')\n",
        "\n",
        "    for genus in genera:\n",
        "      full_text.append(f'- Genus: {genus} includes these languages: \\n-- ')\n",
        "      genus_matches = [language for language in family_matches if language['Genus'] == genus]\n",
        "\n",
        "      language_text = list()\n",
        "      for language in genus_matches:\n",
        "        language_text.append('{} ISO code {} spoken in {}'.format(language['Name'], language['ISO639'], language['Macroarea'] ))\n",
        "      language_text = '\\n-- '.join(language_text)\n",
        "      full_text.append(language_text)\n",
        "\n",
        "    full_text = ''.join(full_text)\n",
        "    #print(full_text)\n",
        "    embeds = embeddings.embed_documents([full_text])\n",
        "    vector = [{\n",
        "      \"id\":str(uuid4()),\n",
        "      \"values\": embeds[0],\n",
        "      \"metadata\": {\"topic\": f\"Data about the {family} family\"}\n",
        "      \"text\": full_text,\n",
        "    }]\n",
        "\n",
        "    index.upsert(vector)\n"
      ],
      "metadata": {
        "id": "SoBlxly8aDkz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_splitter(split_type='html'):\n",
        "  #WALS documents are available in text, html, and json formats\n",
        "  #Chunking plain text seems to give the worst results, likely becaues it's blind to context right now\n",
        "  #html gives better results, because WALS is already structured by headings\n",
        "  #json gives best results, because each text blob can be augemented with some metadata\n",
        "  #this allows every paragraph in a section to carry information about the section name and topic\n",
        "\n",
        "  if split_type == 'text':\n",
        "    tokenizer = tiktoken.get_encoding('p50k_base')\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=400,\n",
        "        chunk_overlap=20,\n",
        "        length_function=tiktoken_len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "  elif split_type == 'html':\n",
        "    headers = [(\"h2\", \"topic\"), (\"h5\", \"introduction\")]\n",
        "    splitter = HTMLHeaderTextSplitter(headers)\n",
        "  elif split_type == 'json':\n",
        "    return None\n",
        "\n",
        "  return splitter\n"
      ],
      "metadata": {
        "id": "1qKaLimIBis1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_new_index = False\n",
        "upsert_metadata = False\n",
        "upsert_chapter_data = True\n",
        "\n",
        "index_name = 'starter-index'\n",
        "split_type = 'json'\n",
        "\n",
        "pc = Pinecone(api_key=userdata.get('PINECONE_TOKEN'))\n",
        "embeddings = LangChainOpenAIEmbeddings(\n",
        "    model =  'text-embedding-ada-002',\n",
        "    openai_api_key=userdata.get('OPENAI_KEY'))\n",
        "\n",
        "if make_new_index:\n",
        "  #https://docs.pinecone.io/docs/manage-indexes#create-a-pod-based-index\n",
        "  pc.delete_index(index_name)\n",
        "  pc.create_index(name=index_name, dimension=1536, metric=\"cosine\", spec=PodSpec(environment=\"gcp-starter\"))\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "if upsert_metadata:\n",
        "  upsert_wals_data(index, embeddings)\n",
        "  upsert_language_data(index, embeddings)\n",
        "\n",
        "\n",
        "if upsert_chapter_data:\n",
        "  chapter_metadata = load_chapter_metadata('/content/drive/MyDrive/WALS/chapter_metadata.csv')\n",
        "  splitter = get_splitter(split_type)\n",
        "  if split_type == 'text':\n",
        "    text_splitter_create_and_upsert_vectors(index, '/content/drive/MyDrive/WALS/', chapter_metadata, splitter, embeddings, subdirectory='chapters_html')\n",
        "  elif split_type == 'html':\n",
        "    html_splitter_create_and_upsert_vectors(index, '/content/drive/MyDrive/WALS/chapters_html', chapter_metadata, splitter, embeddings)\n",
        "  elif split_type == 'json':\n",
        "    json_splitter_create_and_upsert_vectors(index, '/content/drive/MyDrive/WALS/chapters_json', chapter_metadata, embeddings)"
      ],
      "metadata": {
        "id": "3Ic1-pR_6BFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d35d9d-b9a3-46bd-bad6-9873d621d13a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 chapter_1.json\n",
            "2 chapter_10.json\n",
            "3 chapter_100.json\n",
            "4 chapter_101.json\n",
            "5 chapter_102.json\n",
            "6 chapter_103.json\n",
            "7 chapter_104.json\n",
            "8 chapter_105.json\n",
            "9 chapter_106.json\n",
            "10 chapter_107.json\n",
            "11 chapter_108.json\n",
            "12 chapter_109.json\n",
            "13 chapter_11.json\n",
            "14 chapter_110.json\n",
            "15 chapter_111.json\n",
            "16 chapter_112.json\n",
            "17 chapter_113.json\n",
            "18 chapter_114.json\n",
            "19 chapter_115.json\n",
            "20 chapter_116.json\n",
            "21 chapter_117.json\n",
            "22 chapter_118.json\n",
            "23 chapter_119.json\n",
            "24 chapter_12.json\n",
            "25 chapter_120.json\n",
            "26 chapter_121.json\n",
            "27 chapter_122.json\n",
            "28 chapter_123.json\n",
            "29 chapter_124.json\n",
            "30 chapter_125.json\n",
            "31 chapter_126.json\n",
            "32 chapter_128.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "33 chapter_129.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "34 chapter_13.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "35 chapter_130.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "36 chapter_131.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "37 chapter_132.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "38 chapter_133.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "39 chapter_134.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "40 chapter_135.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "41 chapter_136.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "42 chapter_137.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "43 chapter_139.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "44 chapter_14.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "45 chapter_140.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "46 chapter_141.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "47 chapter_142.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "48 chapter_143.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "49 chapter_144.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "50 chapter_15.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "51 chapter_16.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "52 chapter_17.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "53 chapter_18.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "54 chapter_19.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "55 chapter_2.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "56 chapter_21.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "57 chapter_23.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = LangChainPinecone(index, embeddings, text_key='text')"
      ],
      "metadata": {
        "id": "nmppztFPEbkg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LangChainChatOpenAI(\n",
        "    openai_api_key=userdata.get('OPENAI_KEY'),\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.9\n",
        ")"
      ],
      "metadata": {
        "id": "h083MvTuTQqy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "PPVmJN1yhoZC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s1kELVjnBKIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preamble = \"\"\"You are an expert on the World Atlas of Language Structures, also called WALS.\n",
        "Assume that any questions you are asked are referring to WALS, so if someone says Chatper 12\n",
        "they mean Chapter 12 in WALS, if they ask about verbal morphology they really mean what does\n",
        "WALS say about verbal morphology. Act like a helpful librarian who knows about WALS.\"\"\"\n",
        "query = 'Help me find information about velar nasals'\n",
        "response = qa.invoke('\\n'.join([preamble, query]))\n",
        "print(response['result'])"
      ],
      "metadata": {
        "id": "y7FXD_xATmPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3b5235-f379-4507-ad18-510bffb72895"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 10 of the World Atlas of Language Structures (WALS) is titled \"The Velar Nasal\" authored by Gregory D.S. Anderson. This chapter focuses on the linguistic subfield of phonology. If you are looking for information specifically about velar nasals in language structures, Chapter 10 of WALS would be a good place to start.\n"
          ]
        }
      ]
    }
  ]
}