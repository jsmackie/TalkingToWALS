{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Oyn2Hp-XhDxIZqoXLYhLRtlvvYzT1Wm9",
      "authorship_tag": "ABX9TyPr3d/HYFwEg/lSVrLbQvIP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsmackie/TalkingToWALS/blob/mainline/HidingBehindWALS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpMvHAYRQf9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9656d97b-8853-4cf7-b667-fbb1050eb09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.0/211.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    cohere \\\n",
        "    langchain \\\n",
        "    tiktoken \\\n",
        "    pinecone-client \\\n",
        "    langchain-openai \\\n",
        "    langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from uuid import uuid4\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "from pinecone import Pinecone, PodSpec, PineconeApiException\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
        "from langchain_openai import ChatOpenAI as LangChainChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings as LangChainOpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore as LangChainPinecone\n",
        "\n"
      ],
      "metadata": {
        "id": "QCvpIbFUu5NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(text, disallowed_special=())\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "id": "gdRNyCSLDI1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_chapter_metadata(path):\n",
        "  with open(os.path.join(path)) as f:\n",
        "    headers = f.readline().strip().split(',')\n",
        "    lines = [line.strip().split(',') for line in f]\n",
        "  chapter_metadata = dict()\n",
        "  for i,line in enumerate(lines):\n",
        "    chapter_metadata[i] = {'chapter number': i+1,\n",
        "                          'title': line[headers.index('Title')],\n",
        "                          'authors': line[headers.index('Contributors')],\n",
        "                          'linguistic subfield': line[headers.index('Area')]}\n",
        "  return chapter_metadata"
      ],
      "metadata": {
        "id": "B6ojEvRgJ7OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_splitter_create_and_upsert_vectors(index,\n",
        "                                            path,\n",
        "                                            chapter_metadata,\n",
        "                                            text_splitter,\n",
        "                                            embeddings,\n",
        "                                            batch_limit=100,\n",
        "                                            subdirectory='chapters'):\n",
        "  batch_limit = batch_limit\n",
        "  texts = []\n",
        "  metadatas = []\n",
        "  for i,file in enumerate(os.listdir(os.path.join(path, subdirectory))):\n",
        "    metadata = chapter_metadata[i]\n",
        "    with open(os.path.join(path, subdirectory, file), mode='r') as f:\n",
        "      text = ''.join([line for line in f])\n",
        "    record_texts = text_splitter.split_text(text)\n",
        "    record_metadatas = [{\"chunk\": j, \"text\": text, **metadata}\n",
        "                          for j, text in enumerate(record_texts)]\n",
        "    texts.extend(record_texts)\n",
        "    metadatas.extend(record_metadatas)\n",
        "    if len(texts) >= batch_limit:\n",
        "      ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "      embeds = embeddings.embed_documents(texts)\n",
        "      index.upsert(vectors=zip(ids, embeds, metadatas))\n",
        "      texts = []\n",
        "      metadatas = []\n"
      ],
      "metadata": {
        "id": "qOyU15H3tov7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def html_splitter_create_and_upsert_vectors(index,\n",
        "                                            path,\n",
        "                                            chapter_metadata,\n",
        "                                            html_splitter,\n",
        "                                            embeddings):\n",
        "  batch_limit=100\n",
        "  texts = list()\n",
        "  metadatas = list()\n",
        "  for i,file in enumerate(os.listdir(path)):\n",
        "    #print(file)\n",
        "    if not file.endswith('html'):\n",
        "      continue\n",
        "    html_splits = html_splitter.split_text_from_file(os.path.join(path, file))\n",
        "    #Note: this might be too large, and maybe should be split again with RecursiveTextSplitter?\n",
        "    ids = [str(uuid4()) for _ in range(len(html_splits))]\n",
        "    embeds = embeddings.embed_documents([split.page_content for split in html_splits])\n",
        "    #metadatas = [split.metadata for split in html_splits]\n",
        "    metadatas = {key:value for key,value in chapter_metadata[i].items()}\n",
        "    for split in html_splits:\n",
        "      metadatas.update(split.metadata)\n",
        "    record_metadatas = [{\"chunk\": j, \"text\": split.page_content, **metadatas}\n",
        "                          for j, split in enumerate(html_splits)]\n",
        "    index.upsert(vectors=zip(ids, embeds, record_metadatas))"
      ],
      "metadata": {
        "id": "acB4OJyb7Uiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def html_prechunked_create_and_upsert_vectors(index, path, chapter_metadata, embeddings):\n",
        "    for j, file in enumerate(os.listdir(path)):\n",
        "      with open(os.path.join(path, file), encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "      chunks = text.split('\\n-----\\n')\n",
        "      print(f'On Chapter {j}')\n",
        "      ids = [str(uuid4()) for _ in range(len(chunks))]\n",
        "      metadata = chapter_metadata[j]\n",
        "      embeds = embeddings.embed_documents([chunk for chunk in chunks])\n",
        "      record_metadatas = [{\"chunk\": j,\n",
        "                             \"text\": chunk,\n",
        "                             **metadata}\n",
        "                              for (j, chunk) in enumerate(chunks)]\n",
        "      try:\n",
        "        index.upsert(vectors=zip(ids, embeds, record_metadatas))\n",
        "      except PineconeApiException:\n",
        "        print('Pinecone exception occured. Chunk is probably bigger than 2MB! Ignoring.')\n",
        "      #index.upsert(vectors=zip(ids, embeds, record_metadatas))"
      ],
      "metadata": {
        "id": "G5iiUsEne_T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def json_splitter_create_and_upsert_vectors(index, path, chapter_metadata, embeddings):\n",
        "  texts = list()\n",
        "  for i,file in enumerate(sorted(os.listdir(path))):\n",
        "    if not file.endswith('.json'):\n",
        "      continue\n",
        "    print(i, file)\n",
        "    metadatas = {key:value for key,value in chapter_metadata[i].items()}\n",
        "    text_preamble = ''.join(['Hints: ',\n",
        "                            ','.join([f'{k}={v}' for (k,v) in metadatas.items()]),\n",
        "                            '\\n'])\n",
        "    with open(os.path.join(path, file), encoding='utf-8', mode='r') as f:\n",
        "      json_file = json.load(f)\n",
        "    for blob in json_file:\n",
        "      chapter_metadata[i]['section'] = list()\n",
        "      try:\n",
        "        chapter_metadata[i]['section'].append(blob['section'].strip())\n",
        "      except KeyError:\n",
        "        chapter_metadata[i]['section'].append('1. Introduction')\n",
        "      for p in blob['paragraphs']:\n",
        "        texts.append('\\n'.join([text_preamble, p['text']]))\n",
        "    embeds = embeddings.embed_documents(texts)\n",
        "\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    records = [{\"chunk\": j, \"text\": text, **metadatas}\n",
        "                          for j, text in enumerate(texts)]\n",
        "    try:\n",
        "      index.upsert(vectors=zip(ids, embeds, records))\n",
        "    except PineconeApiException:\n",
        "      print('Pinecone exception occured. File is bigger than 2MB! Ignoring.')"
      ],
      "metadata": {
        "id": "0eGMOtWJZvyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsert_wals_data(index, embeddings):\n",
        "  #Get table of contents and other general metadata about WALS\n",
        "  with open('/content/drive/MyDrive/WALS/chapter_metadata.csv', encoding='utf-8') as file:\n",
        "    file_content = file.read()\n",
        "  embeds = embeddings.embed_documents([file_content])\n",
        "  vector = [{\n",
        "      \"id\":str(uuid4()),\n",
        "      \"values\": embeds[0],\n",
        "      \"metadata\": {\"topic\": \"table of contents\"}\n",
        "    }]\n",
        "  index.upsert(vector)"
      ],
      "metadata": {
        "id": "bfLN1SomhnZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upsert_language_data(index, embeddings):\n",
        "  with open('/content/drive/MyDrive/WALS/languages.csv') as f:\n",
        "    headers = f.readline().strip().split(',')\n",
        "    rows = [row.strip().split(',') for row in f]\n",
        "  blobs = list()\n",
        "  for row in rows:\n",
        "    blobs.append({headers[j]:value for (j,value) in enumerate(row)})\n",
        "  #There are ~3000 languages, which makes too many individual records\n",
        "  #try grouping them by family\n",
        "\n",
        "  families = list(set([language['Family'] for language in blobs]))\n",
        "  for family in families:\n",
        "    full_text = list()\n",
        "\n",
        "    family_matches = [language for language in blobs if language['Family'] == family]\n",
        "    genera = list(set([language['Genus'] for language in family_matches]))\n",
        "    full_text.append(f'Language Family: {family} has {len(genera)} subdivisions/groups/genera/etc.\\n')\n",
        "\n",
        "    for genus in genera:\n",
        "      full_text.append(f'- Genus: {genus} includes these languages: \\n-- ')\n",
        "      genus_matches = [language for language in family_matches if language['Genus'] == genus]\n",
        "\n",
        "      language_text = list()\n",
        "      for language in genus_matches:\n",
        "        language_text.append('{} ISO code {} spoken in {}'.format(language['Name'], language['ISO639'], language['Macroarea'] ))\n",
        "      language_text = '\\n-- '.join(language_text)\n",
        "      full_text.append(language_text)\n",
        "\n",
        "    full_text = ''.join(full_text)\n",
        "    #print(full_text)\n",
        "    embeds = embeddings.embed_documents([full_text])\n",
        "    vector = [{\n",
        "      \"id\":str(uuid4()),\n",
        "      \"values\": embeds[0],\n",
        "      \"metadata\": {\"topic\": f\"Data about the {family} family\",\n",
        "                   \"text\": full_text}\n",
        "    }]\n",
        "\n",
        "    index.upsert(vector)\n"
      ],
      "metadata": {
        "id": "SoBlxly8aDkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_splitter(split_type='html'):\n",
        "  #WALS documents are available in text, html, and json formats\n",
        "  #Chunking plain text seems to give the worst results, likely becaues it's blind to context right now\n",
        "  #html gives better results, because WALS is already structured by headings\n",
        "  #json gives best results, because each text blob can be augemented with some metadata\n",
        "  #this allows every paragraph in a section to carry information about the section name and topic\n",
        "\n",
        "  if split_type == 'text':\n",
        "    tokenizer = tiktoken.get_encoding('p50k_base')\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=400,\n",
        "        chunk_overlap=20,\n",
        "        length_function=tiktoken_len,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "  elif split_type == 'html':\n",
        "    headers = [(\"h2\", \"topic\"), (\"h5\", \"introduction\")]\n",
        "    splitter = HTMLHeaderTextSplitter(headers)\n",
        "  else:\n",
        "    splitter = None\n",
        "\n",
        "  return splitter\n"
      ],
      "metadata": {
        "id": "1qKaLimIBis1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_vectorstore(make_new_index = False,\n",
        "                      upsert_metadata = False,\n",
        "                      upsert_chapter_data = False,\n",
        "                      index_name='starter-index',\n",
        "                      split_type = 'json',\n",
        "                      text_embedding = 'text-embedding-ada-002'):\n",
        "\n",
        "  pc = Pinecone(api_key=userdata.get('PINECONE_TOKEN'))\n",
        "  embeddings = LangChainOpenAIEmbeddings(\n",
        "      model =  text_embedding,\n",
        "      openai_api_key=userdata.get('OPENAI_KEY'))\n",
        "\n",
        "  if make_new_index:\n",
        "    #https://docs.pinecone.io/docs/manage-indexes#create-a-pod-based-index\n",
        "    pc.delete_index(index_name)\n",
        "    pc.create_index(name=index_name, dimension=1536, metric=\"cosine\", spec=PodSpec(environment=\"gcp-starter\"))\n",
        "  index = pc.Index(index_name)\n",
        "\n",
        "  if upsert_metadata:\n",
        "    upsert_wals_data(index, embeddings)\n",
        "    #upsert_language_data(index, embeddings)\n",
        "\n",
        "\n",
        "  if upsert_chapter_data:\n",
        "    chapter_metadata = load_chapter_metadata('/content/drive/MyDrive/WALS/chapter_metadata.csv')\n",
        "    if split_type == 'text':\n",
        "      splitter = get_splitter(split_type)\n",
        "      text_splitter_create_and_upsert_vectors(index, '/content/drive/MyDrive/WALS/', chapter_metadata, splitter, embeddings, subdirectory='chapters_html')\n",
        "    elif split_type == 'html':\n",
        "      splitter = get_splitter(split_type)\n",
        "      html_splitter_create_and_upsert_vectors(index, '/content/drive/MyDrive/WALS/chapters_html', chapter_metadata, splitter, embeddings)\n",
        "    elif split_type == 'json':\n",
        "      json_splitter_create_and_upsert_vectors(index, '/content/drive/MyDrive/WALS/chapters_json', chapter_metadata, embeddings)\n",
        "    elif split_type == 'html_prechunk':\n",
        "      html_prechunked_create_and_upsert_vectors(index, '/content/drive/MyDrive/WALS/html_chunks', chapter_metadata, embeddings)\n",
        "\n",
        "  return index, embeddings"
      ],
      "metadata": {
        "id": "3Ic1-pR_6BFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_agent():\n",
        "  index, embeddings = setup_vectorstore(make_new_index=False,\n",
        "                                      upsert_metadata=False,\n",
        "                                      upsert_chapter_data=False)\n",
        "  vectorstore = LangChainPinecone(index, embeddings, text_key='text')\n",
        "\n",
        "  llm = LangChainChatOpenAI(\n",
        "      openai_api_key=userdata.get('OPENAI_KEY'),\n",
        "      model_name='gpt-3.5-turbo',\n",
        "      temperature=0.9\n",
        "  )\n",
        "\n",
        "  agent = RetrievalQA.from_chain_type(\n",
        "      llm=llm,\n",
        "      retriever=vectorstore.as_retriever()\n",
        "  )\n",
        "  return agent"
      ],
      "metadata": {
        "id": "nmppztFPEbkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WALS():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.agent = self.setup_agent()\n",
        "\n",
        "  def TalkToWALS(self, query):\n",
        "    preamble = \"\"\"You are an expert on the World Atlas of Language Structures, also called WALS.\n",
        "    Assume that any questions you are asked are referring to WALS, so if someone says Chatper 12\n",
        "    they mean Chapter 12 in WALS, if they ask about verbal morphology they really mean what does\n",
        "    WALS say about verbal morphology. Act like a helpful librarian who knows about WALS.\"\"\"\n",
        "    response = self.agent.invoke('\\n'.join([preamble, query]))\n",
        "    print(response['result'])\n",
        "\n",
        "  def setup_agent(self):\n",
        "    index, embeddings = setup_vectorstore(make_new_index=False,\n",
        "                                        upsert_metadata=False,\n",
        "                                        upsert_chapter_data=False)\n",
        "    vectorstore = LangChainPinecone(index, embeddings, text_key='text')\n",
        "\n",
        "    llm = LangChainChatOpenAI(\n",
        "        openai_api_key=userdata.get('OPENAI_KEY'),\n",
        "        model_name='gpt-3.5-turbo',\n",
        "        temperature=0.9\n",
        "    )\n",
        "\n",
        "    agent = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever()\n",
        "    )\n",
        "    return agent\n"
      ],
      "metadata": {
        "id": "MtrTSBqwFFDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index, embeddings = setup_vectorstore(make_new_index=False,\n",
        "                  upsert_metadata=False,\n",
        "                  upsert_chapter_data=False,\n",
        "                  split_type='html_prechunk')"
      ],
      "metadata": {
        "id": "SnFPbZyDlGh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wals = WALS()\n"
      ],
      "metadata": {
        "id": "PPVmJN1yhoZC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e77270-176a-466c-afd2-1eb584fb21bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ian Maddieson is a prominent linguist who has contributed to several chapters in the World Atlas of Language Structures (WALS). He is known for his work in phonetics and phonology. Specifically, Maddieson was responsible for writing Chapter 138 in WALS, which focuses on the phonological shape of a lexical item expressing a specific concept related to a specific agricultural product. If you have any more specific questions regarding Ian Maddieson's contributions or any other chapters in WALS, feel free to ask!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wals.TalkToWALS('Give me a summary of Chapter 145')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLWHny85vAHP",
        "outputId": "fa855dc0-c32d-4b59-c03b-38f758519f9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but I do not have information on Chapter 145 of the World Atlas of Language Structures (WALS).\n"
          ]
        }
      ]
    }
  ]
}