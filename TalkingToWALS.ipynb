{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Oyn2Hp-XhDxIZqoXLYhLRtlvvYzT1Wm9",
      "authorship_tag": "ABX9TyP84SvTcQ38VnisGFpvscdx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsmackie/TalkingToWALS/blob/mainline/TalkingToWALS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VpMvHAYRQf9N"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    cohere \\\n",
        "    langchain \\\n",
        "    tiktoken \\\n",
        "    pinecone-client \\\n",
        "    langchain-openai \\\n",
        "    langchain-pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from uuid import uuid4\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "from pinecone import Pinecone, PodSpec, PineconeApiException\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter\n",
        "from langchain_openai import ChatOpenAI as LangChainChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings as LangChainOpenAIEmbeddings\n",
        "from langchain_pinecone import Pinecone as LangChainPinecone\n",
        "\n"
      ],
      "metadata": {
        "id": "QCvpIbFUu5NN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(text, disallowed_special=())\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "id": "gdRNyCSLDI1A"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_chapter_metadata(path):\n",
        "  with open(os.path.join(path)) as f:\n",
        "    headers = f.readline().strip().split(',')\n",
        "    lines = [line.strip().split(',') for line in f]\n",
        "  chapter_metadata = dict()\n",
        "  for i,line in enumerate(lines):\n",
        "    chapter_metadata[i] = {'chapter number': i+1,\n",
        "                          'title': line[headers.index('Title')],\n",
        "                          'authors': line[headers.index('Contributors')],\n",
        "                          'linguistic subfield': line[headers.index('Area')]}\n",
        "  return chapter_metadata"
      ],
      "metadata": {
        "id": "B6ojEvRgJ7OC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_splitter_create_and_upsert_vectors(index,\n",
        "                                            path,\n",
        "                                            chapter_metadata,\n",
        "                                            text_splitter,\n",
        "                                            embeddings,\n",
        "                                            batch_limit=100,\n",
        "                                            subdirectory='chapters'):\n",
        "  batch_limit = batch_limit\n",
        "  texts = []\n",
        "  metadatas = []\n",
        "  for i,file in enumerate(os.listdir(os.path.join(path, subdirectory))):\n",
        "    metadata = chapter_metadata[i]\n",
        "    with open(os.path.join(path, subdirectory, file), mode='r') as f:\n",
        "      text = ''.join([line for line in f])\n",
        "    record_texts = text_splitter.split_text(text)\n",
        "    record_metadatas = [{\"chunk\": j, \"text\": text, **metadata}\n",
        "                          for j, text in enumerate(record_texts)]\n",
        "    texts.extend(record_texts)\n",
        "    metadatas.extend(record_metadatas)\n",
        "    if len(texts) >= batch_limit:\n",
        "      ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "      embeds = embeddings.embed_documents(texts)\n",
        "      index.upsert(vectors=zip(ids, embeds, metadatas))\n",
        "      texts = []\n",
        "      metadatas = []\n"
      ],
      "metadata": {
        "id": "qOyU15H3tov7"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def html_splitter_create_and_upsert_vectors(index,\n",
        "                                            path,\n",
        "                                            chapter_metadata,\n",
        "                                            html_splitter,\n",
        "                                            embeddings):\n",
        "  batch_limit=100\n",
        "  texts = list()\n",
        "  metadatas = list()\n",
        "  for i,file in enumerate(os.listdir(path)):\n",
        "    #print(file)\n",
        "    if not file.endswith('html'):\n",
        "      continue\n",
        "    html_splits = html_splitter.split_text_from_file(os.path.join(path, file))\n",
        "    #Note: this might be too large, and maybe should be split again with RecursiveTextSplitter\n",
        "    ids = [str(uuid4()) for _ in range(len(html_splits))]\n",
        "    embeds = embeddings.embed_documents([split.page_content for split in html_splits])\n",
        "    #metadatas = [split.metadata for split in html_splits]\n",
        "    metadatas = {key:value for key,value in chapter_metadata[i].items()}\n",
        "    for split in html_splits:\n",
        "      metadatas.update(split.metadata)\n",
        "    record_metadatas = [{\"chunk\": j, \"text\": split.page_content, **metadatas}\n",
        "                          for j, split in enumerate(html_splits)]\n",
        "    index.upsert(vectors=zip(ids, embeds, record_metadatas))"
      ],
      "metadata": {
        "id": "acB4OJyb7Uiw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def json_splitter_create_and_upsert_vectors(index, path, chapter_metadata, embeddings):\n",
        "  texts = list()\n",
        "  for i,file in enumerate(sorted(os.listdir(path))):\n",
        "    if not file.endswith('.json'):\n",
        "      continue\n",
        "    print(i, file)\n",
        "    metadatas = {key:value for key,value in chapter_metadata[i].items()}\n",
        "    text_preamble = ''.join(['Hints: ',\n",
        "                            ','.join([f'{k}={v}' for (k,v) in metadatas.items()]),\n",
        "                            '\\n'])\n",
        "    with open(os.path.join(path, file), encoding='utf-8', mode='r') as f:\n",
        "      json_file = json.load(f)\n",
        "    for blob in json_file:\n",
        "      chapter_metadata[i]['section'] = list()\n",
        "      try:\n",
        "        chapter_metadata[i]['section'].append(blob['section'].strip())\n",
        "      except KeyError:\n",
        "        chapter_metadata[i]['section'].append('1. Introduction')\n",
        "      for p in blob['paragraphs']:\n",
        "        texts.append('\\n'.join([text_preamble, p['text']]))\n",
        "      #texts.append(blob['paragraphs'][0]['text'])\n",
        "    embeds = embeddings.embed_documents(texts)\n",
        "\n",
        "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "    records = [{\"chunk\": j, \"text\": text, **metadatas}\n",
        "                          for j, text in enumerate(texts)]\n",
        "    try:\n",
        "      index.upsert(vectors=zip(ids, embeds, records))\n",
        "    except PineconeApiException:\n",
        "      print('Pinecone exception occured. File is bigger than 2MB! Igoring.')"
      ],
      "metadata": {
        "id": "0eGMOtWJZvyo"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_embedding_model = 'text-embedding-ada-002'\n",
        "embeddings = LangChainOpenAIEmbeddings(\n",
        "    model = text_embedding_model,\n",
        "    openai_api_key=userdata.get('OPENAI_KEY'))\n",
        "\n",
        "split_type = 'json'\n",
        "\n",
        "if split_type == 'text':\n",
        "  tokenizer = tiktoken.get_encoding('p50k_base')\n",
        "  splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=400,\n",
        "      chunk_overlap=20,\n",
        "      length_function=tiktoken_len,\n",
        "      separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "      )\n",
        "elif split_type == 'html':\n",
        "  headers = [(\"h2\", \"topic\"), (\"h5\", \"introduction\")]#, (\"h6\", \"example data\")]\n",
        "  splitter = HTMLHeaderTextSplitter(headers)\n",
        "elif split_type == 'json':\n",
        "  pass #no extra formatting required\n"
      ],
      "metadata": {
        "id": "1qKaLimIBis1"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_new_index = True\n",
        "\n",
        "pc = Pinecone(api_key=userdata.get('PINECONE_TOKEN'))\n",
        "index_name = 'starter-index'\n",
        "\n",
        "if make_new_index:\n",
        "  pc.delete_index(index_name)\n",
        "  pc.create_index(name=index_name, dimension=1536, metric=\"cosine\", spec=PodSpec(environment=\"gcp-starter\"))\n",
        "  index = pc.Index(index_name)\n",
        "  chapter_metadata = load_chapter_metadata('/content/drive/MyDrive/WALS/chapter_metadata.csv')\n",
        "  if split_type == 'text':\n",
        "    text_splitter_create_and_upsert_vectors(index, '/content/drive/MyDrive/WALS/', chapter_metadata, splitter, embeddings, subdirectory='chapters_html')\n",
        "  elif split_type == 'html':\n",
        "    html_splitter_create_and_upsert_vectors(index, '/content/drive/MyDrive/WALS/chapters_html', chapter_metadata, splitter, embeddings)\n",
        "  elif split_type == 'json':\n",
        "    json_splitter_create_and_upsert_vectors(index, '/content/drive/MyDrive/WALS/chapters_json', chapter_metadata, embeddings)\n",
        "else:\n",
        "  index = pc.Index(index_name)\n",
        "#See also: https://docs.pinecone.io/docs/manage-indexes#create-a-pod-based-index\n"
      ],
      "metadata": {
        "id": "3Ic1-pR_6BFL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1d8f2eda-7929-4c59-9da3-a95160e87de4"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 chapter_1.json\n",
            "2 chapter_10.json\n",
            "3 chapter_100.json\n",
            "4 chapter_101.json\n",
            "5 chapter_102.json\n",
            "6 chapter_103.json\n",
            "7 chapter_104.json\n",
            "8 chapter_105.json\n",
            "9 chapter_106.json\n",
            "10 chapter_107.json\n",
            "11 chapter_108.json\n",
            "12 chapter_109.json\n",
            "13 chapter_11.json\n",
            "14 chapter_110.json\n",
            "15 chapter_111.json\n",
            "16 chapter_112.json\n",
            "17 chapter_113.json\n",
            "18 chapter_114.json\n",
            "19 chapter_115.json\n",
            "20 chapter_116.json\n",
            "21 chapter_117.json\n",
            "22 chapter_118.json\n",
            "23 chapter_119.json\n",
            "24 chapter_12.json\n",
            "25 chapter_120.json\n",
            "26 chapter_121.json\n",
            "27 chapter_122.json\n",
            "28 chapter_123.json\n",
            "29 chapter_124.json\n",
            "30 chapter_125.json\n",
            "31 chapter_126.json\n",
            "32 chapter_128.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "33 chapter_129.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "34 chapter_13.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "35 chapter_130.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "36 chapter_131.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "37 chapter_132.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "38 chapter_133.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "39 chapter_134.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "40 chapter_135.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "41 chapter_136.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "42 chapter_137.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "43 chapter_139.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "44 chapter_14.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "45 chapter_140.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "46 chapter_141.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "47 chapter_142.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "48 chapter_143.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "49 chapter_144.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "50 chapter_15.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "51 chapter_16.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "52 chapter_17.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "53 chapter_18.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "54 chapter_19.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "55 chapter_2.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "56 chapter_21.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "57 chapter_23.json\n",
            "Pinecone exception occured. File is bigger than 2MB! Igoring.\n",
            "58 chapter_24.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 4138 (char 4137)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-db98403be5f5>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mhtml_splitter_create_and_upsert_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/WALS/chapters_html'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchapter_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0msplit_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'json'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mjson_splitter_create_and_upsert_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/drive/MyDrive/WALS/chapters_json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchapter_metadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-b0b939fd777c>\u001b[0m in \u001b[0;36mjson_splitter_create_and_upsert_vectors\u001b[0;34m(index, path, chapter_metadata, embeddings)\u001b[0m\n\u001b[1;32m     10\u001b[0m                             '\\n'])\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0mchapter_metadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'section'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 4138 (char 4137)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/WALS/chapter_metadata.csv', encoding='utf-8') as file:\n",
        "    file_content = file.read()\n",
        "embeds = embeddings.embed_documents([file_content])\n",
        "vector = {\n",
        "      \"id\":str(uuid4()),\n",
        "      \"values\": embeds[0][0],\n",
        "      \"metadata\": {\"topic\": \"table of contents\"}\n",
        "    }\n",
        "#index.upsert(vector)"
      ],
      "metadata": {
        "id": "gmHZuypF1sS9"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = LangChainPinecone(index, embeddings, text_key='text')"
      ],
      "metadata": {
        "id": "nmppztFPEbkg"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LangChainChatOpenAI(\n",
        "    openai_api_key=userdata.get('OPENAI_KEY'),\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.8\n",
        ")"
      ],
      "metadata": {
        "id": "h083MvTuTQqy"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "PPVmJN1yhoZC"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'Who wrote chapter 28?'\n",
        "response = qa.invoke(query)\n",
        "print(response['result'])"
      ],
      "metadata": {
        "id": "y7FXD_xATmPm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c9b40b-e431-4023-f8df-52d6909a5ff6"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chapter 28 titled \"Reduplication\" was written by Carl Rubino.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ISPT9E3vy93t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a4bLGogsu7Z_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}