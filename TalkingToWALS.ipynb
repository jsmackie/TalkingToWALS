{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Oyn2Hp-XhDxIZqoXLYhLRtlvvYzT1Wm9",
      "authorship_tag": "ABX9TyPfMny52o75hL0wWH8qfE2N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsmackie/TalkingToWALS/blob/mainline/TalkingToWALS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VpMvHAYRQf9N"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "    langchain \\\n",
        "    tiktoken \\\n",
        "    pinecone-client \\\n",
        "    langchain-openai \\\n",
        "    langchain-pinecone\n",
        "    #langchain_pinecone"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from uuid import uuid4\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "from pinecone import Pinecone, PodSpec\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI as LangChainChatOpenAI\n",
        "from langchain_openai import OpenAIEmbeddings as LangChainOpenAIEmbeddings\n",
        "from langchain_pinecone import Pinecone as LangChainPinecone\n",
        "\n"
      ],
      "metadata": {
        "id": "QCvpIbFUu5NN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_chapter_metadata(path):\n",
        "  with open(os.path.join(path)) as f:\n",
        "    headers = f.readline().strip().split(',')\n",
        "    lines = [line.strip().split(',') for line in f]\n",
        "  chapter_metadata = dict()\n",
        "  for i,line in enumerate(lines):\n",
        "    chapter_metadata[i] = {'title': line[headers.index('Name')],\n",
        "                          'authors': line[headers.index('Contributor')],\n",
        "                          'source': line[headers.index('Citation')],\n",
        "                          'topic': line[headers.index('Topic')]}\n",
        "  return chapter_metadata"
      ],
      "metadata": {
        "id": "B6ojEvRgJ7OC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_upsert_vectors(path,\n",
        "                              chapter_metadata,\n",
        "                              text_splitter,\n",
        "                              embeddings,\n",
        "                              batch_limit=100):\n",
        "  batch_limit = batch_limit\n",
        "  texts = []\n",
        "  metadatas = []\n",
        "  for i,file in enumerate(os.listdir(os.path.join(path, 'chapters'))):\n",
        "    metadata = chapter_metadata[i]\n",
        "    with open(os.path.join(path, 'chapters', file), mode='r') as f:\n",
        "      text = ''.join([line for line in f])\n",
        "    record_texts = text_splitter.split_text(text)\n",
        "    record_metadatas = [{\"chunk\": j, \"text\": text, **metadata}\n",
        "                        for j, text in enumerate(record_texts)]\n",
        "    texts.extend(record_texts)\n",
        "    metadatas.extend(record_metadatas)\n",
        "    if len(texts) >= batch_limit:\n",
        "      ids = [str(uuid4()) for _ in range(len(texts))]\n",
        "      embeds = embeddings.embed_documents(texts)\n",
        "      index.upsert(vectors=zip(ids, embeds, metadatas))\n",
        "      texts = []\n",
        "      metadatas = []\n"
      ],
      "metadata": {
        "id": "qOyU15H3tov7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(text,disallowed_special=())\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "id": "gdRNyCSLDI1A"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialized a bunch of useful objets: tokenizer, splitter, embeddings\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('p50k_base')\n",
        "text_embedding_model = 'text-embedding-ada-002'\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=400,\n",
        "    chunk_overlap=20,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "embeddings = LangChainOpenAIEmbeddings(\n",
        "    model = text_embedding_model,\n",
        "    openai_api_key=userdata.get('OPENAI_KEY')\n",
        ")\n"
      ],
      "metadata": {
        "id": "1qKaLimIBis1"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Change this value depending on whether the index already exists\n",
        "new_index = False\n",
        "\n",
        "pc = Pinecone(api_key=userdata.get('PINECONE_TOKEN'))\n",
        "index_name = 'starter-index'\n",
        "\n",
        "if new_index:\n",
        "  pc.delete_index(index_name)\n",
        "  pc.create_index(name=index_name, dimension=1536, metric=\"cosine\", spec=PodSpec(environment=\"gcp-starter\"))\n",
        "  chapter_metadata = load_chapter_metadata('/content/drive/MyDrive/WALS/chapter_metadata.csv')\n",
        "  create_and_upsert_vectors('/content/drive/MyDrive/WALS/', chapter_metadata, text_splitter, embeddings)\n",
        "index = pc.Index(index_name)\n",
        "#See also: https://docs.pinecone.io/docs/manage-indexes#create-a-pod-based-index\n"
      ],
      "metadata": {
        "id": "3Ic1-pR_6BFL"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vectorstore = LangChainPinecone(index, embeddings, text_key='text')"
      ],
      "metadata": {
        "id": "nmppztFPEbkg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LangChainChatOpenAI(\n",
        "    openai_api_key=userdata.get('OPENAI_KEY'),\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.5\n",
        ")"
      ],
      "metadata": {
        "id": "h083MvTuTQqy"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "PPVmJN1yhoZC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What is WALS?'\n",
        "response = qa.invoke(query)\n",
        "print(response['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7FXD_xATmPm",
        "outputId": "c401ba05-1a8a-48e1-d262-c133bdc277f8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WALS stands for the World Atlas of Language Structures. It is an atlas that provides maps showing the geographical distribution of structural linguistic features. It is the first feature atlas on a worldwide scale and focuses on abstract features of language systems that can be compared across unrelated languages. WALS is used by linguists interested in linguistic typology to study the ways in which languages vary structurally and the limits to this variation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a4bLGogsu7Z_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}